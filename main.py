#!/usr/bin/env python3
"""
11880.com E-Mail-Scraper fÃ¼r Hausverwaltungen
HauptausfÃ¼hrungsdatei mit CLI-Interface
"""

import asyncio
import argparse
import sys
from pathlib import Path
from datetime import datetime

# Add src to path
sys.path.append(str(Path(__file__).parent / "src"))

from src.scraper.main_scraper import MainScraper
from src.utils.logging_config import setup_logging, get_logger
from src.utils.browser_manager import BrowserManager
from src.export.csv_exporter import CSVExporter

logger = get_logger(__name__)


async def main():
    """Hauptfunktion fÃ¼r den Scraper"""

    # Argument parsing
    parser = argparse.ArgumentParser(
        description="11880.com E-Mail-Scraper fÃ¼r Hausverwaltungen",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Beispiele:
  python main.py                                      # Standard-Scraping fÃ¼r Hausverwaltungen in DÃ¼sseldorf
  python main.py --location "KÃ¶ln"                   # Scraping in KÃ¶ln
  python main.py --search-term "Immobilienmakler"    # Andere Branche
  python main.py --max-pages 10                      # Nur 10 Seiten scrapen
  python main.py --test                              # Test-Modus mit wenigen EintrÃ¤gen
        """,
    )

    parser.add_argument(
        "--location",
        default="DÃ¼sseldorf",
        help="Zielstadt fÃ¼r die Suche (default: DÃ¼sseldorf)",
    )

    parser.add_argument(
        "--search-term",
        default="Hausverwaltungen",
        help="Suchbegriff fÃ¼r Unternehmen (default: Hausverwaltungen)",
    )

    parser.add_argument(
        "--max-pages",
        type=int,
        default=50,
        help="Maximale Anzahl der zu scrapenden Seiten (default: 50)",
    )

    parser.add_argument(
        "--output-dir",
        default="data",
        help="Ausgabe-Verzeichnis fÃ¼r CSV-Dateien (default: data)",
    )

    parser.add_argument(
        "--log-level",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        default="INFO",
        help="Log-Level (default: INFO)",
    )

    parser.add_argument(
        "--headless", action="store_true", help="Browser im Headless-Modus ausfÃ¼hren"
    )

    parser.add_argument(
        "--test", action="store_true", help="Test-Modus: Scrapt nur wenige EintrÃ¤ge"
    )

    parser.add_argument(
        "--no-emails",
        action="store_true",
        help="Keine E-Mail-Extraktion durchfÃ¼hren (nur Firmendaten)",
    )

    parser.add_argument(
        "--delay",
        type=float,
        default=None,
        help="Basis-Delay zwischen Requests in Sekunden",
    )

    args = parser.parse_args()

    # Logging Setup
    setup_logging(level=args.log_level)

    logger.info("=" * 60)
    logger.info("ğŸš€ 11880.com E-Mail-Scraper gestartet")
    logger.info("=" * 60)
    logger.info(f"ğŸ“ Location: {args.location}")
    logger.info(f"ğŸ” Search Term: {args.search_term}")
    logger.info(f"ğŸ“„ Max Pages: {args.max_pages}")
    logger.info(f"ğŸ’¾ Output Dir: {args.output_dir}")
    logger.info(f"ğŸ–¥ï¸ Headless: {args.headless}")
    logger.info(f"ğŸ§ª Test Mode: {args.test}")
    logger.info(f"ğŸ“§ Extract Emails: {not args.no_emails}")

    if args.test:
        logger.info("âš ï¸ Test-Modus aktiviert - Nur wenige EintrÃ¤ge werden extrahiert")
        args.max_pages = 2  # Begrenzt auf 2 Seiten im Test-Modus

    # Browser Manager initialisieren
    browser_manager = BrowserManager(headless=args.headless)

    # CSV Exporter initialisieren
    csv_exporter = CSVExporter(output_directory=args.output_dir)

    # Main Scraper initialisieren
    scraper = MainScraper(browser_manager=browser_manager, csv_exporter=csv_exporter)

    try:
        start_time = datetime.now()

        # Scraping durchfÃ¼hren
        results = await scraper.scrape_companies(
            search_term=args.search_term,
            location=args.location,
            max_pages=args.max_pages,
            extract_emails=not args.no_emails,
            delay_override=args.delay,
            test_mode=args.test,
        )

        end_time = datetime.now()
        duration = end_time - start_time

        # Ergebnisse anzeigen
        logger.info("=" * 60)
        logger.info("âœ… SCRAPING ABGESCHLOSSEN")
        logger.info("=" * 60)
        logger.info(f"â±ï¸ Dauer: {duration}")
        logger.info(f"ğŸ“Š Gefundene Unternehmen: {results.get('total_companies', 0)}")
        logger.info(f"ğŸ“§ Extrahierte E-Mails: {results.get('emails_found', 0)}")
        logger.info(f"ğŸ’¾ CSV-Datei: {results.get('output_file', 'N/A')}")

        if results.get("errors", 0) > 0:
            logger.warning(f"âš ï¸ Aufgetretene Fehler: {results['errors']}")

        # Performance-Statistiken
        if duration.total_seconds() > 0:
            companies_per_minute = (
                results.get("total_companies", 0) / duration.total_seconds()
            ) * 60
            logger.info(
                f"âš¡ Performance: {companies_per_minute:.1f} Unternehmen/Minute"
            )

        logger.info("=" * 60)

        return 0

    except KeyboardInterrupt:
        logger.info("\nğŸ›‘ Scraping durch Benutzer unterbrochen")
        return 1

    except Exception as e:
        logger.error(f"âŒ Unerwarteter Fehler: {e}")
        logger.exception("Stacktrace:")
        return 1

    finally:
        # Cleanup
        await scraper.cleanup()
        logger.info("ğŸ§¹ Cleanup abgeschlossen")


def print_banner():
    """Zeigt ein schÃ¶nes Banner"""
    banner = """
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                              â•‘
    â•‘                ğŸ¢ 11880.com E-Mail-Scraper ğŸ¢               â•‘
    â•‘                                                              â•‘
    â•‘              Sammelt E-Mail-Adressen von                    â•‘
    â•‘              Hausverwaltungen automatisch                   â•‘
    â•‘                                                              â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    print(banner)


if __name__ == "__main__":
    # Banner anzeigen
    print_banner()

    # Event Loop Policy fÃ¼r Windows setzen
    if sys.platform.startswith("win"):
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

    # Asyncio ausfÃ¼hren
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\nğŸ›‘ Programm unterbrochen")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ Kritischer Fehler: {e}")
        sys.exit(1)
